{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e9664ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get data\n",
    "import itertools\n",
    "import math\n",
    "import random\n",
    "from statistics import mean\n",
    "from typing import Tuple, List, Dict, Set\n",
    "\n",
    "# Helper types for the clustering task.\n",
    "Embedding = Tuple[float, ...]\n",
    "ClusterId = int\n",
    "ClusteredDataset = Dict[ClusterId, List[Embedding]]\n",
    "\n",
    "GOLD_STANDARD_CLUSTER_STD = 0.2\n",
    "DIMENSIONS = 32\n",
    "EPOCHS = 10\n",
    "N_CLUSTERS = 4\n",
    "N_SAMPLES = 1000\n",
    "\n",
    "def generate_data(\n",
    "    samples: int = N_SAMPLES, dimensions: int = DIMENSIONS, clusters: int = N_CLUSTERS\n",
    ") -> Tuple[ClusteredDataset, List[Embedding]]:\n",
    "    \"\"\"Generates a random dataset and flat list of embeddings.\"\"\"\n",
    "    # Distributions.\n",
    "    distributions = []\n",
    "    for _ in range(clusters):\n",
    "        mean = random.random()\n",
    "        distributions.append((mean, GOLD_STANDARD_CLUSTER_STD))\n",
    "\n",
    "    # Generate samples.\n",
    "    dataset: ClusteredDataset = {}\n",
    "    embeddings = []\n",
    "    for _ in range(samples):\n",
    "        cluster_id = random.randrange(0, clusters)\n",
    "        mean, std = distributions[cluster_id]\n",
    "        embedding = tuple(random.normalvariate(mean, std) for _ in range(dimensions))\n",
    "        dataset.setdefault(cluster_id, []).append(embedding)\n",
    "        embeddings.append(embedding)\n",
    "    random.shuffle(embeddings)\n",
    "    data={}\n",
    "    for i,data_point in enumerate(embeddings):\n",
    "        key_='id_'+str(i)\n",
    "        data[key_] = data_point\n",
    "    \n",
    "    return dataset,data#, embeddings\n",
    "\n",
    "gold_standard, embeddings = generate_data(clusters=N_CLUSTERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d313d7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Centroid:\n",
    "    def __init__(self,location):\n",
    "        self.location = location\n",
    "        self.closest_points = set()\n",
    "\n",
    "def get_k_means(feature_map,num_features_per_id,k):\n",
    "    \"\"\"Get the right centroid for data points\n",
    "    input: feature_map all the data points (dictionary -k:id,v:feature for k-th key)\n",
    "    num_features_per_tag: dimensionality of features\n",
    "    k = number of clusters\n",
    "    return list of centroids object for given clusters\"\"\"\n",
    "    \n",
    "    #randomly initialised centroid - choose ids\n",
    "    initial_centroid_ids = random.sample(sorted(list(feature_map.keys())),k)\n",
    "    centroids = [Centroid(feature_map[id]) for id in initial_centroid_ids]\n",
    "     # now update the centroid in a iterative manner\n",
    "            # 2 loops 1- number of eppochs, 2 - for all data points\n",
    "    for ii in range(50):\n",
    "        for id_,feature in feature_map.items():\n",
    "            closest_id_distance = float('inf')\n",
    "            closest_centroid = None\n",
    "            for centroid in centroids:\n",
    "                feature_to_centroid_distance = get_distance(centroid.location,feature)\n",
    "                if feature_to_centroid_distance<closest_id_distance:\n",
    "                    closest_id_distance = feature_to_centroid_distance\n",
    "                    closest_centroid = centroid\n",
    "            closest_centroid.closest_points.add(id_)\n",
    "            # Updating centroid is done\n",
    "        # Now update the centroids\n",
    "#         print(ii)\n",
    "        for centroid in centroids:\n",
    "            #calculate new centroid as per the data in centroid\n",
    "            centroid.locaton = get_average_distance(centroid,num_features_per_id,feature_map)\n",
    "            # clear the data \n",
    "            if ii!=19:\n",
    "                centroid.closest_points.clear()\n",
    "    return centroids\n",
    "            \n",
    "def get_average_distance(centroid,num_features_per_id,feature_map):\n",
    "    \"\"\"calculate the average distance from the features\n",
    "    input: centroid object\n",
    "    return averaged distance of that features\"\"\"\n",
    "    total_records = len(centroid.closest_points)\n",
    "    centroid_avg = []\n",
    "    for feature_ith in range(num_features_per_id):\n",
    "        data_point =0\n",
    "        for record in centroid.closest_points:\n",
    "            data_point +=feature_map[record][feature_ith]\n",
    "        centroid_avg.append(data_point/num_features_per_id)  \n",
    "    return centroid_avg\n",
    "                    \n",
    "def get_distance(centroid,feature):\n",
    "    \"\"\"get manhattan distance\"\"\"\n",
    "    absolute_diff = []\n",
    "    for centroid_dim,feature_dim in zip(centroid,feature):\n",
    "        absolute_diff.append(abs(centroid_dim-feature_dim))\n",
    "    return sum(absolute_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d4494c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_means_centroid = get_k_means(embeddings,DIMENSIONS,N_CLUSTERS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9d645b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e043d72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
