{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff6885aa",
   "metadata": {},
   "source": [
    "### Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944f1bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import Counter, defaultdict\n",
    "import itertools\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1dc42cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spcify the stemmer and stopwords\n",
    "porter = PorterStemmer()\n",
    "stop_words = stopwords.words(\"english\")\n",
    "\n",
    "# https://www.kaggle.com/rmisra/news-category-dataset\n",
    "file = \"/home/user2/Machine-learning-without-any-libraries/NB/News_Category_Dataset_v2.json\"\n",
    "\n",
    "\n",
    "\n",
    "def clean_data(sentence):\n",
    "    \"\"\"clean text data by removing stop words, steming, and removing punctuations\n",
    "    args: text string\n",
    "    return : cleaned text dictionary counter\"\"\"\n",
    "    words = [\n",
    "        porter.stem(word)\n",
    "        for word in word_tokenize(sentence)\n",
    "        if word.lower() not in stop_words and word.isalpha()\n",
    "    ]\n",
    "    \n",
    "    return Counter(words)\n",
    "\n",
    "\n",
    "def get_data(file,train_test_thresh=.9):\n",
    "    \"\"\"Read file and get the data for each category with cleaned text in a dictionary format.\n",
    "        \n",
    "        args: input file name to read, train_test_split(float<1)\n",
    "        output: A list of dictioanry key : categry. Value: list of count dictionaries \n",
    "        (key : cleaned word, values: count of that word)\"\"\"\n",
    "    \n",
    "    # Open file and read line by line and append to a list of dictionaries for each record\n",
    "    data = [json.loads(l) for l in open(file, \"r\")]\n",
    "    # We need only news category, headline and short_descriptin for the analysis\n",
    "    # We'll merge the heading and short_description as text for the category \n",
    "    keys_ = [\"category\", \"headline\", \"short_description\"] \n",
    "    # filter the data as per the specified keys_\n",
    "    data = [{k: val for k, val in record.items() if k in keys_} for record in data]\n",
    "    # get the unique tags\n",
    "    categories = list(set([record[\"category\"] for record in data]))\n",
    "    # get data for each category as list\n",
    "    data_clenced = {tag: [] for tag in categories}\n",
    "    # update the data_clenced dict\n",
    "    [\n",
    "        data_clenced[l[\"category\"]].append(\n",
    "            clean_data(l[\"headline\"] + \" \" + l[\"short_description\"])\n",
    "        )\n",
    "        for l in data\n",
    "    ]\n",
    "    train_data={}\n",
    "    test_data={}\n",
    "    for k, v in data_clenced.items():\n",
    "        train_data[k]=v[: int(len(v)*train_test_thresh)]\n",
    "        test_data[k]=v[int(len(v)*train_test_thresh):]\n",
    "    return train_data,test_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "13165e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def IDF(corpus, unique_words):\n",
    "    \"\"\" get the idf from the corpus and unique words\n",
    "    args: corpus(list of list) and unique words\n",
    "    output: dictionary for each word with idf value\"\"\"\n",
    "    idf_dict={}\n",
    "    N=len(corpus)\n",
    "    for i in unique_words:\n",
    "        count=0\n",
    "        for sen_list in corpus:\n",
    "            if i in sen_list:\n",
    "                count=count+1\n",
    "            idf_dict[i]=(math.log((1+N)/(count+1)))+1\n",
    "#     print(idf_dict)\n",
    "    return idf_dict \n",
    "\n",
    "def get_vocab_and_idf(whole_data):\n",
    "    \"\"\"get vocabulary and idf of vocabulary\n",
    "    args: corpus(list of list)\n",
    "    output: vocabulary(dict key:word, v:index of word), idf_value for\n",
    "    each word\n",
    "    \"\"\"\n",
    "    unique_words = set()\n",
    "    if isinstance(whole_data, (list,)):\n",
    "#         for x in whole_data:\n",
    "#             for y in x:\n",
    "#             if len(x)<2:\n",
    "#                 continue\n",
    "#             unique_words.add(x)\n",
    "        unique_words=set(list(itertools.chain(*whole_data)))\n",
    "        unique_words = sorted(list(unique_words))\n",
    "        vocab = {j:i for i,j in enumerate(unique_words)}\n",
    "        Idf_values_of_all_unique_words=IDF(whole_data,unique_words)\n",
    "#         print(Idf_values_of_all_unique_words)\n",
    "    return vocab, Idf_values_of_all_unique_words\n",
    "\n",
    "\n",
    "def get_tf():\n",
    "    \"\"\"get the tf\n",
    "    args : none\n",
    "    output: nested dictionary, key: word(in vocab), values: dictionary(k:category,v:tf_value)\"\"\"\n",
    "    tf_dict=defaultdict(lambda:{tag:0 for tag in train_data.keys()})\n",
    "    for w in idf_of_vocabulary.keys():\n",
    "        for tag,v in train_data.items():\n",
    "            num=0\n",
    "            freq=0\n",
    "            for i in v:\n",
    "                num += len(i)\n",
    "                freq += i[w]\n",
    "                \n",
    "            p = (freq+1)/(num+2)\n",
    "            tf_dict[w][tag] = p\n",
    "    return tf_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5b5e74a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data,test_data=get_data(file)\n",
    "\n",
    "# Get corpus(list of list) all the words in the text in train test\n",
    "# corpus = list(itertools.chain(*list(itertools.chain(*train_data.values()))))\n",
    "corpus = list(itertools.chain(*train_data.values()))\n",
    "# corpus =[]\n",
    "# [l.extend(list(i.keys())) for i in list(itertools.chain(*test_data.values()))]\n",
    "Vocabulary, idf_of_vocabulary=get_vocab_and_idf(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "50abba6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_dict=get_tf()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fe12ca5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85\n"
     ]
    }
   ],
   "source": [
    "corr = 0\n",
    "tot =0\n",
    "for k_ in test_data.keys():\n",
    "    pro_pred={k:1 for k,_ in test_data.items()}\n",
    "    for i in test_data[k_]:\n",
    "        sample_data = i\n",
    "        for k in test_data.keys():\n",
    "            for w in sample_data:\n",
    "                idf = idf_of_vocabulary[w] if w in idf_of_vocabulary else 1/len(idf_of_vocabulary)\n",
    "#                 tf = tf_dict[w][k] if w in tf_dict else 1/    \n",
    "                pro_pred[k] += math.log(idf)*tf_dict[w][k]\n",
    "                if max(pro_pred,key=pro_pred.get)==k_:\n",
    "                    corr +=1\n",
    "                tot +=1\n",
    "#         print(max(pro_pred,key=pro_pred.get),k_)\n",
    "print(round(corr*100/tot))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31fe548",
   "metadata": {},
   "source": [
    "## Done!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82f0d9e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
